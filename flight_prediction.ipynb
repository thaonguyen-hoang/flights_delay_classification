{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in all required libraries\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf \n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import math \n",
    "import statistics as stat\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell'\n",
    "\n",
    "# set up the spark configuartion\n",
    "spark_conf = SparkConf()\\\n",
    ".setMaster('local[2]')\\\n",
    ".set('spark.sql.session.timeZone', 'UTC')\n",
    "\n",
    "# start the spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(conf=spark_conf) \\\n",
    "    .appName(\"Flight Data Streaming\") \\\n",
    "    .getOrCreate() \n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connect to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the topic for this session\n",
    "topic = 'flight-topic'\n",
    "\n",
    "# read the stream sent by the producer\n",
    "flight_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"127.0.0.1:9092\") \\\n",
    "    .option(\"subscribe\", topic) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing the schema of the data stream\n",
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the key, value kafka data stream to string format\n",
    "flight_df = flight_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split value string into a list\n",
    "flight_df = flight_df.selectExpr(\"split(value, ',') AS values\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format each field to a suitable data type\n",
    "flight_df = flight_df.selectExpr(\"CAST(values[0] AS INT)AS YEAR\", \n",
    "                   \"CAST(values[1] AS INT) AS MONTH\",\n",
    "                   \"CAST(values[2] AS INT) AS DAY\",\n",
    "                   \"CAST(values[3] AS INT) AS DAY_OF_WEEK\",\n",
    "                   \"values[4] AS AIRLINE\",\n",
    "                   \"CAST(values[5] AS INT) AS FLIGHT_NUMBER\",\n",
    "                   \"values[6] AS TAIL_NUMBER\",\n",
    "                   \"values[7] AS ORIGIN_AIRPORT\",\n",
    "                   \"values[8] AS DESTINATION_AIRPORT\",\n",
    "                   \"CAST(values[9] AS INT) AS SCHEDULED_DEPARTURE\",\n",
    "                   \"CAST(values[10] AS INT) AS DEPARTURE_TIME\",\n",
    "                   \"CAST(values[11] AS INT) AS DEPARTURE_DELAY\",\n",
    "                   \"CAST(values[12] AS INT) AS TAXI_OUT\",\n",
    "                   \"CAST(values[13] AS INT) AS WHEELS_OFF\",\n",
    "                   \"CAST(values[14] AS INT) AS SCHEDULED_TIME\",\n",
    "                   \"CAST(values[15] AS INT) AS ELAPSED_TIME\",\n",
    "                   \"CAST(values[16] AS INT) AS AIR_TIME\",\n",
    "                   \"CAST(values[17] AS INT) AS DISTANCE\",\n",
    "                   \"CAST(values[18] AS INT) AS WHEELS_ON\",\n",
    "                   \"CAST(values[19] AS INT) AS TAXI_IN\",\n",
    "                   \"CAST(values[20] AS INT) AS SCHEDULED_ARRIVAL\",\n",
    "                   \"CAST(values[21] AS INT) AS ARRIVAL_TIME\",\n",
    "                   \"CAST(values[22] AS INT) AS ARRIVAL_DELAY\",\n",
    "                   \"CAST(values[23] AS INT) AS DIVERTED\",\n",
    "                   \"CAST(values[24] AS INT) AS CANCELLED\",\n",
    "                   \"values[25] AS CANCELLATION_REASON\",\n",
    "                   \"CAST(values[26] AS INT) AS AIR_SYSTEM_DELAY\",\n",
    "                   \"CAST(values[27] AS INT) AS SECURITY_DELAY\",\n",
    "                   \"CAST(values[28] AS INT) AS AIRLINE_DELAY\",\n",
    "                   \"CAST(values[29] AS INT) AS LATE_AIRCRAFT_DELAY\",\n",
    "                   \"CAST(values[30] AS INT) AS WEATHER_DELAY\"\n",
    "\n",
    ")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- YEAR: integer (nullable = true)\n",
      " |-- MONTH: integer (nullable = true)\n",
      " |-- DAY: integer (nullable = true)\n",
      " |-- DAY_OF_WEEK: integer (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- FLIGHT_NUMBER: integer (nullable = true)\n",
      " |-- TAIL_NUMBER: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT: string (nullable = true)\n",
      " |-- DESTINATION_AIRPORT: string (nullable = true)\n",
      " |-- SCHEDULED_DEPARTURE: integer (nullable = true)\n",
      " |-- DEPARTURE_TIME: integer (nullable = true)\n",
      " |-- DEPARTURE_DELAY: integer (nullable = true)\n",
      " |-- TAXI_OUT: integer (nullable = true)\n",
      " |-- WHEELS_OFF: integer (nullable = true)\n",
      " |-- SCHEDULED_TIME: integer (nullable = true)\n",
      " |-- ELAPSED_TIME: integer (nullable = true)\n",
      " |-- AIR_TIME: integer (nullable = true)\n",
      " |-- DISTANCE: integer (nullable = true)\n",
      " |-- WHEELS_ON: integer (nullable = true)\n",
      " |-- TAXI_IN: integer (nullable = true)\n",
      " |-- SCHEDULED_ARRIVAL: integer (nullable = true)\n",
      " |-- ARRIVAL_TIME: integer (nullable = true)\n",
      " |-- ARRIVAL_DELAY: integer (nullable = true)\n",
      " |-- DIVERTED: integer (nullable = true)\n",
      " |-- CANCELLED: integer (nullable = true)\n",
      " |-- CANCELLATION_REASON: string (nullable = true)\n",
      " |-- AIR_SYSTEM_DELAY: integer (nullable = true)\n",
      " |-- SECURITY_DELAY: integer (nullable = true)\n",
      " |-- AIRLINE_DELAY: integer (nullable = true)\n",
      " |-- LATE_AIRCRAFT_DELAY: integer (nullable = true)\n",
      " |-- WEATHER_DELAY: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print schema\n",
    "flight_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of columns deemed to be not useful for our models\n",
    "removed_columns = ['CANCELLATION_REASON','AIR_SYSTEM_DELAY','SECURITY_DELAY',\n",
    "                     'AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function that removes unwated columns\n",
    "def eliminate_columns(removed_columns, df):\n",
    "    \n",
    "    # drop the unwated columns\n",
    "    df = df.drop(*removed_columns)\n",
    "    \n",
    "    # return the modified dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the function that removes unwated columns\n",
    "flightsRawDf = eliminate_columns(removed_columns, flight_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with na values\n",
    "flightsDf = flightsRawDf.na.drop(\"any\")\n",
    "\n",
    "# drop rows with null values\n",
    "flightsDf = flightsDf.dropna(\"any\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make prediction using trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 1: Decision Tree and arrival delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model 1\n",
    "dt_arr_model = PipelineModel.load('models/dt_arr')\n",
    "# apply model 1\n",
    "dt_arr_pred = dt_arr_model.transform(flightsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predicted dataframe to in-memory table \n",
    "see_results1 = dt_arr_pred \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"predictions1\") \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop query\n",
    "see_results1.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         probability|\n",
      "+----------+--------------------+\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       1.0|[0.40971299495083...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       1.0|[0.14783537177247...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       1.0|[0.40971299495083...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display prediction\n",
    "from IPython.display import clear_output, display\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(spark.sql('SELECT prediction, probability FROM predictions1').show())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 2: Decision Tree and departure delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model 2 \n",
    "dt_dept_model = PipelineModel.load('models/dt_dep')\n",
    "\n",
    "# apply model 2\n",
    "dt_dept_pred = dt_dept_model.transform(flightsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predicted dataframe to the sink \n",
    "see_results2 = dt_dept_pred \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"predictions2\") \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop query\n",
    "see_results2.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         probability|\n",
      "+----------+--------------------+\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       1.0|[0.14783537177247...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display prediction\n",
    "from IPython.display import clear_output, display\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(spark.sql('SELECT prediction, probability FROM predictions2').show())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 3: Gradient Boost Tree and arrival delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model 3 \n",
    "gbt_arr_model = PipelineModel.load('models/gbt_arr')\n",
    "\n",
    "# apply model 3\n",
    "gbt_arr_pred = gbt_arr_model.transform(flightsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predicted dataframe to the sink \n",
    "see_results3 = gbt_arr_pred \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"predictions3\") \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop query\n",
    "see_results3.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         probability|\n",
      "+----------+--------------------+\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       1.0|[0.14783537177247...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       1.0|[0.40971299495083...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display prediction\n",
    "from IPython.display import clear_output, display\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(spark.sql('SELECT prediction, probability FROM predictions3').show())\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model 4: Gradient Boost Tree and departure delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model 4 \n",
    "gbt_dept_model = PipelineModel.load('models/abt_dep')\n",
    "\n",
    "# apply model 4\n",
    "gbt_dept_pred = gbt_dept_model.transform(flightsDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write predicted dataframe to the sink \n",
    "see_results4 = gbt_dept_pred \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"predictions4\") \\\n",
    "    .trigger(processingTime='2 seconds') \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop query\n",
    "see_results4.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|prediction|         probability|\n",
      "+----------+--------------------+\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       1.0|[0.14783537177247...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.70365735671538...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "|       0.0|[0.83846775707005...|\n",
      "+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display prediction\n",
    "from IPython.display import clear_output, display\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(spark.sql('SELECT prediction, probability FROM predictions4').show())\n",
    "    time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6c4729a6600565384ecf5c5203830e69782e8735ec2854311c86ca4cb48f9a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
